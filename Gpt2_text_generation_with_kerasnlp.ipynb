{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Itskedarjasud/Data-Dreamers-5201/blob/main/Gpt2_text_generation_with_kerasnlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OA_MzEGxjfd"
      },
      "source": [
        "# GPT2 Text Generation with KerasNLP\n",
        "\n",
        "**Author:** Chen Qian<br>\n",
        "**Date created:** 2024/04/1<br>\n",
        "**Last modified:** 2024/06/21<br>\n",
        "**Description:** Use KerasNLP GPT2 model and `samplers` to do text generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWPpVmYIxjfe"
      },
      "source": [
        "In this tutorial, you will learn to use [KerasNLP](https://keras.io/keras_nlp/) to load a\n",
        "pre-trained Large Language Model (LLM) - [GPT-2 model](https://openai.com/research/better-language-models)\n",
        "(originally invented by OpenAI), finetune it to a specific text style, and\n",
        "generate text based on users' input (also known as prompt). You will also learn\n",
        "how GPT2 adapts quickly to non-English languages, such as Chinese."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jbfuN1uxjff"
      },
      "source": [
        "##  Before we begin\n",
        "\n",
        "Colab offers different kinds of runtimes. Make sure to go to **Runtime ->\n",
        "Change runtime type** and choose the GPU Hardware Accelerator runtime\n",
        "(which should have >12G host RAM and ~15G GPU RAM) since you will finetune the\n",
        "GPT-2 model. Running this tutorial on CPU runtime will take hours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ljgtQT7xjff"
      },
      "source": [
        "## Install KerasNLP, Choose Backend and Import Dependencies\n",
        "\n",
        "This examples uses [Keras 3](https://keras.io/keras_3/) to work in any of\n",
        "`\"tensorflow\"`, `\"jax\"` or `\"torch\"`. Support for Keras 3 is baked into\n",
        "KerasNLP, simply change the `\"KERAS_BACKEND\"` environment variable to select\n",
        "the backend of your choice. We select the JAX backend below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jvWReAkfxjff",
        "outputId": "61d9bf14-69c6-41ab-a5f0-9dbbf82b6098",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/keras-team/keras-nlp.git -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vkF7Normxjfg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n",
        "\n",
        "import keras_nlp\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYsrwFefxjfg"
      },
      "source": [
        "## Introduction to Generative Large Language Models (LLMs)\n",
        "\n",
        "Large language models (LLMs) are a type of machine learning models that are\n",
        "trained on a large corpus of text data to generate outputs for various natural\n",
        "language processing (NLP) tasks, such as text generation, question answering,\n",
        "and machine translation.\n",
        "\n",
        "Generative LLMs are typically based on deep learning neural networks, such as\n",
        "the [Transformer architecture](https://arxiv.org/abs/1706.03762) invented by\n",
        "Google researchers in 2017, and are trained on massive amounts of text data,\n",
        "often involving billions of words. These models, such as Google [LaMDA](https://blog.google/technology/ai/lamda/)\n",
        "and [PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html),\n",
        "are trained with a large dataset from various data sources which allows them to\n",
        "generate output for many tasks. The core of Generative LLMs is predicting the\n",
        "next word in a sentence, often referred as **Causal LM Pretraining**. In this\n",
        "way LLMs can generate coherent text based on user prompts. For a more\n",
        "pedagogical discussion on language models, you can refer to the\n",
        "[Stanford CS324 LLM class](https://stanford-cs324.github.io/winter2022/lectures/introduction/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g48bcS1axjfg"
      },
      "source": [
        "## Introduction to KerasNLP\n",
        "\n",
        "Large Language Models are complex to build and expensive to train from scratch.\n",
        "Luckily there are pretrained LLMs available for use right away. [KerasNLP](https://keras.io/keras_nlp/)\n",
        "provides a large number of pre-trained checkpoints that allow you to experiment\n",
        "with SOTA models without needing to train them yourself.\n",
        "\n",
        "KerasNLP is a natural language processing library that supports users through\n",
        "their entire development cycle. KerasNLP offers both pretrained models and\n",
        "modularized building blocks, so developers could easily reuse pretrained models\n",
        "or stack their own LLM.\n",
        "\n",
        "In a nutshell, for generative LLM, KerasNLP offers:\n",
        "\n",
        "- Pretrained models with `generate()` method, e.g.,\n",
        "    `keras_nlp.models.GPT2CausalLM` and `keras_nlp.models.OPTCausalLM`.\n",
        "- Sampler class that implements generation algorithms such as Top-K, Beam and\n",
        "    contrastive search. These samplers can be used to generate text with\n",
        "    custom models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXQpqsoxxjfg"
      },
      "source": [
        "## Load a pre-trained GPT-2 model and generate some text\n",
        "\n",
        "KerasNLP provides a number of pre-trained models, such as [Google\n",
        "Bert](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)\n",
        "and [GPT-2](https://openai.com/research/better-language-models). You can see\n",
        "the list of models available in the [KerasNLP repository](https://github.com/keras-team/keras-nlp/tree/master/keras_nlp/models).\n",
        "\n",
        "It's very easy to load the GPT-2 model as you can see below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nvmKYtewxjfg"
      },
      "outputs": [],
      "source": [
        "# To speed up training and generation, we use preprocessor of length 128\n",
        "# instead of full length 1024.\n",
        "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    sequence_length=128,\n",
        ")\n",
        "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
        "    \"gpt2_base_en\", preprocessor=preprocessor\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm8Slif7xjfh"
      },
      "source": [
        "Once the model is loaded, you can use it to generate some text right away. Run\n",
        "the cells below to give it a try. It's as simple as calling a single function\n",
        "*generate()*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "un7H4Pinxjfh",
        "outputId": "7b6a104d-12f6-42dd-9a65-e945a22ea61f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "My trip to Yosemite was one of my favorite things in my adult life. It was a day I spent in a beautiful valley, surrounded by beautiful mountains that made me want to explore.\n",
            "\n",
            "I was in Yosemite at the time, but I was not alone in my passion for exploring the mountains. There are many places that offer a unique experience, and I love the feeling of being able to explore those.\n",
            "\n",
            "It's a good feeling, but it can also feel like a little bit of a distraction. There is a reason why I love Yosemite.\n",
            "\n",
            "The first time I visited Yosemite, I was in my 20s, which was a pretty big change for me. I wasn't a teenager, but I was still in high school, so I was a little bit nervous.\n",
            "\n",
            "When I was in college, I was really into hiking and exploring the mountains. The only way I could really enjoy the view was to hike in the back of the car. I loved it\n",
            "TOTAL TIME ELAPSED: 12.52s\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "\n",
        "output = gpt2_lm.generate(\"My trip to Yosemite was\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou9xYngtxjfh"
      },
      "source": [
        "Try another one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gKuxQIVLxjfh",
        "outputId": "d9211c57-09e4-40bb-e904-4deead0b26c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "That Italian restaurant is called \"The Italian Grill\" is located on the corner of South and East Street in downtown San Francisco, where you'll find a lot of Italian food, a lot of wine, and some great food. It has a large selection of Italian dishes and some of them are quite good.\n",
            "\n",
            "You can see a video of this Italian restaurant below.\n",
            "TOTAL TIME ELAPSED: 2.64s\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "\n",
        "output = gpt2_lm.generate(\"That Italian restaurant is\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"TOTAL TIME ELAPSED: {end - start:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDT-J-5Gxjfh"
      },
      "source": [
        "Notice how much faster the second call is. This is because the computational\n",
        "graph is [XLA compiled](https://www.tensorflow.org/xla) in the 1st run and\n",
        "re-used in the 2nd behind the scenes.\n",
        "\n",
        "The quality of the generated text looks OK, but we can improve it via\n",
        "fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66kBhgLYxjfh"
      },
      "source": [
        "## More on the GPT-2 model from KerasNLP\n",
        "\n",
        "Next up, we will actually fine-tune the model to update its parameters, but\n",
        "before we do, let's take a look at the full set of tools we have to for working\n",
        "with for GPT2.\n",
        "\n",
        "The code of GPT2 can be found\n",
        "[here](https://github.com/keras-team/keras-nlp/blob/master/keras_nlp/models/gpt2/).\n",
        "Conceptually the `GPT2CausalLM` can be hierarchically broken down into several\n",
        "modules in KerasNLP, all of which have a *from_preset()* function that loads a\n",
        "pretrained model:\n",
        "\n",
        "- `keras_nlp.models.GPT2Tokenizer`: The tokenizer used by GPT2 model, which is a\n",
        "    [byte-pair encoder](https://huggingface.co/course/chapter6/5?fw=pt).\n",
        "- `keras_nlp.models.GPT2CausalLMPreprocessor`: the preprocessor used by GPT2\n",
        "    causal LM training. It does the tokenization along with other preprocessing\n",
        "    works such as creating the label and appending the end token.\n",
        "- `keras_nlp.models.GPT2Backbone`: the GPT2 model, which is a stack of\n",
        "    `keras_nlp.layers.TransformerDecoder`. This is usually just referred as\n",
        "    `GPT2`.\n",
        "- `keras_nlp.models.GPT2CausalLM`: wraps `GPT2Backbone`, it multiplies the\n",
        "    output of `GPT2Backbone` by embedding matrix to generate logits over\n",
        "    vocab tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QSw8maAxjfh"
      },
      "source": [
        "## Finetune on imdb_reviews dataset\n",
        "\n",
        "Now you have the knowledge of the GPT-2 model from KerasNLP, you can take one\n",
        "step further to finetune the model so that it generates text in a specific\n",
        "style, short or long, strict or casual. In this tutorial, we will use reddit\n",
        "dataset for example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbfL7cMsxjfh"
      },
      "source": [
        "Let's take a look inside sample data from the imdb_reviews TensorFlow Dataset. There\n",
        "are two features:\n",
        "\n",
        "- **__document__**: text of the post.\n",
        "- **__title__**: the title."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Load the IMDB dataset\n",
        "imdb_ds, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
        "train_ds, test_ds = imdb_ds['train'], imdb_ds['test']\n"
      ],
      "metadata": {
        "id": "po8c5nLU0SM4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the Dataset:\n",
        "You need to preprocess the text data, including tokenization and padding, similarly to the previous example."
      ],
      "metadata": {
        "id": "7wQKQOUqB8KI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tokenizer parameters\n",
        "vocab_size = 10000\n",
        "oov_token = '<OOV>'\n",
        "max_length = 256\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
        "\n",
        "# Extract texts and labels\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "\n",
        "for text, label in tfds.as_numpy(train_ds):\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "\n",
        "for text, label in tfds.as_numpy(test_ds):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "\n",
        "# Fit the tokenizer on training texts\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# Convert texts to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# Pad the sequences\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n"
      ],
      "metadata": {
        "id": "_YMnf9QV0hbZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create TensorFlow Datasets from the Preprocessed Data"
      ],
      "metadata": {
        "id": "V3nlytzXAkNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_padded, train_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_padded, test_labels))\n",
        "\n",
        "# Shuffle and batch the dataset\n",
        "batch_size = 32\n",
        "train_dataset = train_dataset.shuffle(buffer_size=10000).batch(batch_size, drop_remainder=True)\n",
        "test_dataset = test_dataset.batch(batch_size, drop_remainder=True)\n"
      ],
      "metadata": {
        "id": "H39EQ2qW08Rh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build and Train a Keras Model:\n",
        "Here's an example of a simple sentiment analysis model using an embedding layer, LSTM, and a dense output layer."
      ],
      "metadata": {
        "id": "aGgH3jxpBWAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, GlobalAveragePooling1D\n",
        "\n",
        "# Model parameters\n",
        "embedding_dim = 64\n",
        "lstm_units = 64\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    LSTM(lstm_units),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_dataset, validation_data=test_dataset, epochs=10)\n"
      ],
      "metadata": {
        "id": "0B1WsZC51AQL",
        "outputId": "74ce742b-ae73-4e08-df02-3639d985c3e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m781/781\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 60ms/step - accuracy: 0.5030 - loss: nan - val_accuracy: 0.5030 - val_loss: nan\n",
            "Epoch 2/10\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 70ms/step - accuracy: 0.5053 - loss: nan - val_accuracy: 0.5030 - val_loss: nan\n",
            "Epoch 3/10\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 55ms/step - accuracy: 0.5025 - loss: nan - val_accuracy: 0.5001 - val_loss: nan\n",
            "Epoch 4/10\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 70ms/step - accuracy: 0.5016 - loss: nan - val_accuracy: 0.5001 - val_loss: nan\n",
            "Epoch 5/10\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - accuracy: 0.5040 - loss: nan - val_accuracy: 0.5001 - val_loss: nan\n",
            "Epoch 6/10\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 55ms/step - accuracy: 0.4982 - loss: nan - val_accuracy: 0.5001 - val_loss: nan\n",
            "Epoch 7/10\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 47ms/step - accuracy: 0.5007 - loss: nan - val_accuracy: 0.5001 - val_loss: nan\n",
            "Epoch 8/10\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 79ms/step - accuracy: 0.4989 - loss: nan - val_accuracy: 0.5001 - val_loss: nan\n",
            "Epoch 9/10\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 56ms/step - accuracy: 0.5030 - loss: nan - val_accuracy: 0.5001 - val_loss: nan\n",
            "Epoch 10/10\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 79ms/step - accuracy: 0.4951 - loss: nan - val_accuracy: 0.5001 - val_loss: nan\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e1383f29450>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After fine-tuning is finished, you can again generate text using the same generate() function. This time, the text will be closer to imdb_reviews writing style, and the generated length will be close to our preset length in the training set."
      ],
      "metadata": {
        "id": "IZ2kDWnpCOG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt, model, tokenizer, max_length=100):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='tf')\n",
        "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "prompt1 = \"The movie was fantastic because\"\n",
        "generated_text1 = gpt2_lm.generate(prompt1, max_length=100)\n",
        "print(\"Generated text 1:\")\n",
        "print(generated_text1)\n",
        "\n",
        "# Generate another text with the same or a different prompt\n",
        "prompt2 = \"The storyline was gripping and\"\n",
        "generated_text2 = gpt2_lm.generate(prompt2, max_length=100)\n",
        "print(\"Generated text 2:\")\n",
        "print(generated_text2)\n"
      ],
      "metadata": {
        "id": "qII9Htbc6sHr",
        "outputId": "3008dd22-ae4e-493b-bb53-3a7e92842c7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text 1:\n",
            "The movie was fantastic because it was about two guys in their 20s, and they were trying to make a movie about them. They were trying to get a movie about the guys who were doing it for the first time in a long time and they didn't know what to expect. And I was thinking, this movie is so funny and so funny. And it was a really good movie. And it's funny to think that this is a movie about two guys who are really good at it\n",
            "Generated text 2:\n",
            "The storyline was gripping and the characters were all over the map. But the main character was not. It was not just the character who was being portrayed.\n",
            "\n",
            "I think it is because of this story that I feel like it's been a lot of work. I don't think that the story has gotten better in the last year. The story is still going on. The characters have been getting better, but the story is still going on. I don't know how many of the characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3V9ZMgNxjfi"
      },
      "source": [
        "## Into the Sampling Method\n",
        "\n",
        "In KerasNLP, we offer a few sampling methods, e.g., contrastive search,\n",
        "Top-K and beam sampling. By default, our `GPT2CausalLM` uses Top-k search, but\n",
        "you can choose your own sampling method.\n",
        "\n",
        "Much like optimizer and activations, there are two ways to specify your custom\n",
        "sampler:\n",
        "\n",
        "- Use a string identifier, such as \"greedy\", you are using the default\n",
        "configuration via this way.\n",
        "- Pass a `keras_nlp.samplers.Sampler` instance, you can use custom configuration\n",
        "via this way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "QsDPVPzUxjfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5aa2bb5-6a84-45b4-e186-aa004caa41e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "I like basketball, and I'm a fan of the game. But I don't know how to play it, because it doesn't work for me.\n",
            "\n",
            "The only time I can play basketball in a big city in the U.S., I'm on the court for about three hours. I'm not playing in front of the media, but I do watch games. I'm not going out in front of the media. I'm playing for my team. It's a big deal.\n",
            "\n",
            "I like the way the team plays. They are very smart. They have good players. They are good on both ends of the floor. They are smart. They know how to defend and how to get the shot.\n",
            "\n",
            "They are very smart. I like how they play the game. I like the way they play the game.\n",
            "\n",
            "They have good defense. They have good shooters, and they have good shooters.\n",
            "\n",
            "It's a big difference from what you saw\n",
            "\n",
            "GPT-2 output:\n",
            "I like basketball. I like to play it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to watch it. I like to\n"
          ]
        }
      ],
      "source": [
        "# Use a string identifier.\n",
        "gpt2_lm.compile(sampler=\"top_k\")\n",
        "output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)\n",
        "\n",
        "# Use a `Sampler` instance. `GreedySampler` tends to repeat itself,\n",
        "greedy_sampler = keras_nlp.samplers.GreedySampler()\n",
        "gpt2_lm.compile(sampler=greedy_sampler)\n",
        "\n",
        "output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0MpGL_7xjfi"
      },
      "source": [
        "For more details on KerasNLP `Sampler` class, you can check the code\n",
        "[here](https://github.com/keras-team/keras-nlp/tree/master/keras_nlp/samplers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZzKox_nxjfi"
      },
      "source": [
        "## Finetune on Chinese Poem Dataset\n",
        "\n",
        "We can also finetune GPT2 on non-English datasets. For readers knowing Chinese,\n",
        "this part illustrates how to fine-tune GPT2 on Chinese poem dataset to teach our\n",
        "model to become a poet!\n",
        "\n",
        "Because GPT2 uses byte-pair encoder, and the original pretraining dataset\n",
        "contains some Chinese characters, we can use the original vocab to finetune on\n",
        "Chinese dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Z37jwin4xjfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b940a30-ab4d-4226-c699-082884a2efcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'chinese-poetry' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!# Load chinese poetry dataset.\n",
        "!git clone https://github.com/chinese-poetry/chinese-poetry.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9bgt339xjfi"
      },
      "source": [
        "Load text from the json file. We only use„ÄäÂÖ®ÂîêËØó„Äãfor demo purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "wtjoPsAgxjfi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "poem_collection = []\n",
        "for file in os.listdir(\"chinese-poetry/ÂÖ®ÂîêËØó\"):\n",
        "    if \".json\" not in file or \"poet\" not in file:\n",
        "        continue\n",
        "    full_filename = \"%s/%s\" % (\"chinese-poetry/ÂÖ®ÂîêËØó\", file)\n",
        "    with open(full_filename, \"r\") as f:\n",
        "        content = json.load(f)\n",
        "        poem_collection.extend(content)\n",
        "\n",
        "paragraphs = [\"\".join(data[\"paragraphs\"]) for data in poem_collection]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQvO7CZWxjfj"
      },
      "source": [
        "Let's take a look at sample data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "JXjWDN1Gxjfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0251533b-69f2-44ec-df59-5505334af641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Á©∫Ë∞∑Ë∑´ÁÑ∂ËÅûË∂≥Èü≥ÔºåÊÆ∑Âã§ÈÄöÂÆàËá¥ÂÆ∂Á¶Ω„ÄÇÈô∞Êô¥‰∏çÁàΩÂè∏Êô®‚ñ°Ôºå‚ñ°ÊöÆÂÖ®ÁÑ°Ëµ∑ËàûÂøÉ„ÄÇÂ§¢Áü≠ÈÇ£Â†™Áú†Ë≠¶ÊûïÔºåËÑõÈï∑Á®çÂ∑≤ÊÄØ‚ñ°Ë°æ„ÄÇÂèØÊÜê‰∏ÄÂøµÂ∞§ËøÇÈóäÔºåÊúóË™¶Ë•øÂ±±Â§úÊ∞£ÁÆ¥„ÄÇ\n"
          ]
        }
      ],
      "source": [
        "print(paragraphs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N0VIwLSxjfj"
      },
      "source": [
        "Similar as Reddit example, we convert to TF dataset, and only use partial data\n",
        "to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "I7TE7TmAxjfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f3197fd-602e-4888-ed67-2a6bad028ba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m500/500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 175ms/step - accuracy: 0.2498 - loss: 2.6126\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e1384b79d20>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "train_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(paragraphs)\n",
        "    .batch(16)\n",
        "    .cache()\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Running through the whole dataset takes long, only take `500` and run 1\n",
        "# epochs for demo purposes.\n",
        "train_ds = train_ds.take(500)\n",
        "num_epochs = 1\n",
        "\n",
        "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
        "    5e-4,\n",
        "    decay_steps=train_ds.cardinality() * num_epochs,\n",
        "    end_learning_rate=0.0,\n",
        ")\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "gpt2_lm.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=loss,\n",
        "    weighted_metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "gpt2_lm.fit(train_ds, epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaCG0S6Sxjfj"
      },
      "source": [
        "Let's check the result!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6hN_7-8vxjfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73feeec4-cad7-4fb6-88c6-596d6958c95e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Êò®Â§úÈõ®ÁñèÈ£éÈ™§ÔºåÁµÇÁø∞Áü•Â±±ËêΩËá≥„ÄÇÁü•Ëã•Èù¢Á¥∞Ëá™ÁµÆÔºåÁøªÊõ≤Â†¥Ê∏∏Á∂†„ÄÇÈùûÈ¢®ËêΩÊôØÁü≥ÔºåË©©Â°ûÁ¥´Ê®ôÈ¶≥„ÄÇÊ∏ÖÈ†àÈ¶ôÈ†ªÁ∂†Ôºå\n"
          ]
        }
      ],
      "source": [
        "output = gpt2_lm.generate(\"Êò®Â§úÈõ®ÁñèÈ£éÈ™§\", max_length=200)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqgxSLVWxjfn"
      },
      "source": [
        "Not bad üòÄ"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}